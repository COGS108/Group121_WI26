{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric\n",
    "\n",
    "Instructions: DELETE this cell before you submit via a `git push` to your repo before deadline. This cell is for your reference only and is not needed in your report. \n",
    "\n",
    "Scoring: Out of 10 points\n",
    "\n",
    "- Each Developing  => -2 pts\n",
    "- Each Unsatisfactory/Missing => -4 pts\n",
    "  - until the score is \n",
    "\n",
    "If students address the detailed feedback in a future checkpoint they will earn these points back\n",
    "\n",
    "\n",
    "|                  | Unsatisfactory                                                                                                                                                                                                    | Developing                                                                                                                                                                                              | Proficient                                     | Excellent                                                                                                                              |\n",
    "|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| Data relevance   | Did not have data relevant to their question. Or the datasets don't work together because there is no way to line them up against each other. If there are multiple datasets, most of them have this trouble | Data was only tangentially relevant to the question or a bad proxy for the question. If there are multiple datasets, some of them may be irrelevant or can't be easily combined.                       | All data sources are relevant to the question. | Multiple data sources for each aspect of the project. It's clear how the data supports the needs of the project.                         |\n",
    "| Data description | Dataset or its cleaning procedures are not described. If there are multiple datasets, most have this trouble                                                                                              | Data was not fully described. If there are multiple datasets, some of them are not fully described                                                                                                      | Data was fully described                       | The details of the data descriptions and perhaps some very basic EDA also make it clear how the data supports the needs of the project. |\n",
    "| Data wrangling   | Did not obtain data. They did not clean/tidy the data they obtained.  If there are multiple datasets, most have this trouble                                                                                 | Data was partially cleaned or tidied. Perhaps you struggled to verify that the data was clean because they did not present it well. If there are multiple datasets, some have this trouble | The data is cleaned and tidied.                | The data is spotless and they used tools to visualize the data cleanliness and you were convinced at first glance                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 108 - Data Checkpoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with your team list and their contributions. Note that this will change over the course of the checkpoints\n",
    "\n",
    "This is a modified [CRediT taxonomy of contributions](https://credit.niso.org). For each group member please list how they contributed to this project using these terms:\n",
    "> Analysis, Background research, Conceptualization, Data curation, Experimental investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review & editing\n",
    "\n",
    "Example team list and credits:\n",
    "**Eric Badilla:** Conceptualization, Data curation, Methodology, Writing – original draft \n",
    "\n",
    "**Nishka Vaghela:** Background research, Writing – original draft \n",
    "\n",
    "**Niharika Sapre:** Conceptualization, Writing – original draft \n",
    "\n",
    "**Renee Li:** Methodology, Writing – original draft \n",
    "\n",
    "**Jenny Fu:** Project administration, Writing – original draft"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Question"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background and Prior Work"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: REPLACE the contents of this cell with your work, including any updates to recover points lost in your proposal feedback\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data overview\n",
    "\n",
    "Instructions: REPLACE the contents of this cell with descriptions of your actual datasets.\n",
    "\n",
    "For each dataset include the following information\n",
    "- Dataset #1\n",
    "  - Dataset Name:\n",
    "  - Link to the dataset:\n",
    "  - Number of observations:\n",
    "  - Number of variables:\n",
    "  - Description of the variables most relevant to this project\n",
    "  - Descriptions of any shortcomings this dataset has with repsect to the project\n",
    "- Dataset #2 (if you have more than one!)\n",
    "  - same as above\n",
    "- etc\n",
    "\n",
    "Each dataset deserves either a set of bullet points as above or a few sentences if you prefer that method.\n",
    "\n",
    "If you plan to use multiple datasets, add a few sentences about how you plan to combine these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code every time when you're actively developing modules in .py files.  It's not needed if you aren't making modules\n",
    "#\n",
    "## this code is necessary for making sure that any modules we load are updated here \n",
    "## when their source code .py files are modified\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Download Progress:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading airline-safety.csv:   0%|          | 0.00/1.23k [00:00<?, ?B/s]\u001b[A\n",
      "                                                                           \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: airline-safety.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading bad-drivers.csv:   0%|          | 0.00/1.37k [00:00<?, ?B/s]\u001b[A\n",
      "Overall Download Progress: 100%|██████████| 2/2 [00:00<00:00, 25.02it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded: bad-drivers.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup code -- this only needs to be run once after cloning the repo!\n",
    "# this code downloads the data from its source to the `data/00-raw/` directory\n",
    "# if the data hasn't updated you don't need to do this again!\n",
    "\n",
    "# if you don't already have these packages (you should!) uncomment this line\n",
    "# %pip install requests tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('./modules') # this tells python where to look for modules to import\n",
    "\n",
    "import get_data # this is where we get the function we need to download data\n",
    "\n",
    "# replace the urls and filenames in this list with your actual datafiles\n",
    "# yes you can use Google drive share links or whatever\n",
    "# format is a list of dictionaries; \n",
    "# each dict has keys of \n",
    "#   'url' where the resource is located\n",
    "#   'filename' for the local filename where it will be stored \n",
    "datafiles = [\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/airline-safety/airline-safety.csv', 'filename':'airline-safety.csv'},\n",
    "    { 'url': 'https://raw.githubusercontent.com/fivethirtyeight/data/refs/heads/master/bad-drivers/bad-drivers.csv', 'filename':'bad-drivers.csv'}\n",
    "]\n",
    "\n",
    "get_data.get_raw(datafiles,destination_directory='data/00-raw/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #1 \n",
    "\n",
    "Instructions: \n",
    "1. Change the header from Dataset #1 to something more descriptive of the dataset\n",
    "2. Write a few paragraphs about this dataset. Make sure to cover\n",
    "   1. Describe the important metrics, what units they are in, and giv some sense of what they mean.  For example \"Fasting blood glucose in units of mg glucose per deciliter of blood.  Normal values for healthy individuals range from 70 to 100 mg/dL.  Values 100-125 are prediabetic and values >125mg/dL indicate diabetes. Values <70 indicate hypoglycemia. Fasting idicates the patient hasn't eaten in the last 8 hours.  If blood glucose is >250 or <50 at any time (regardless of the time of last meal) the patient's life may be in immediate danger\"\n",
    "   2. If there are any major concerns with the dataset, describe them. For example \"Dataset is composed of people who are serious enough about eating healthy that they voluntarily downloaded an app dedicated to tracking their eating patterns. This sample is likely biased because of that self-selection. These people own smartphones and may be healthier and may have more disposable income than the average person.  Those who voluntarily log conscientiously and for long amounts of time are also likely even more interested in health than those who download the app and only log a bit before getting tired of it\"\n",
    "3. Use the cell below to \n",
    "    1. load the dataset \n",
    "    2. make the dataset tidy or demonstrate that it was already tidy\n",
    "    3. demonstrate the size of the dataset\n",
    "    4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "    5. find and flag any outliers or suspicious entries\n",
    "    6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "    7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "4. Optionally you can also show some summary statistics for variables that you think are important to the project\n",
    "5. Feel free to add more cells here if that's helpful for you\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE TO LOAD/CLEAN/TIDY/WRANGLE THE DATA GOES HERE\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset #2 \n",
    "\n",
    "See instructions above for Dataset #1.  Feel free to keep adding as many more datasets as you need.  Put each new dataset in its own section just like these. \n",
    "\n",
    "Lastly if you do have multiple datasets, add another section where you demonstrate how you will join, align, cross-reference or whatever to combine data from the different datasets\n",
    "\n",
    "Please note that you can always keep adding more datasets in the future if these datasets you turn in for the checkpoint aren't sufficient.  The goal here is demonstrate that you can obtain and wrangle data.  You are not tied down to only use what you turn in right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123849 entries, 0 to 123848\n",
      "Data columns (total 31 columns):\n",
      " #   Column                      Non-Null Count   Dtype  \n",
      "---  ------                      --------------   -----  \n",
      " 0   job_id                      123849 non-null  int64  \n",
      " 1   company_name                122130 non-null  object \n",
      " 2   title                       123849 non-null  object \n",
      " 3   description                 123842 non-null  object \n",
      " 4   max_salary                  29793 non-null   float64\n",
      " 5   pay_period                  36073 non-null   object \n",
      " 6   location                    123849 non-null  object \n",
      " 7   company_id                  122132 non-null  float64\n",
      " 8   views                       122160 non-null  float64\n",
      " 9   med_salary                  6280 non-null    float64\n",
      " 10  min_salary                  29793 non-null   float64\n",
      " 11  formatted_work_type         123849 non-null  object \n",
      " 12  applies                     23320 non-null   float64\n",
      " 13  original_listed_time        123849 non-null  float64\n",
      " 14  remote_allowed              15246 non-null   float64\n",
      " 15  job_posting_url             123849 non-null  object \n",
      " 16  application_url             87184 non-null   object \n",
      " 17  application_type            123849 non-null  object \n",
      " 18  expiry                      123849 non-null  float64\n",
      " 19  closed_time                 1073 non-null    float64\n",
      " 20  formatted_experience_level  94440 non-null   object \n",
      " 21  skills_desc                 2439 non-null    object \n",
      " 22  listed_time                 123849 non-null  float64\n",
      " 23  posting_domain              83881 non-null   object \n",
      " 24  sponsored                   123849 non-null  int64  \n",
      " 25  work_type                   123849 non-null  object \n",
      " 26  currency                    36073 non-null   object \n",
      " 27  compensation_type           36073 non-null   object \n",
      " 28  normalized_salary           36073 non-null   float64\n",
      " 29  zip_code                    102977 non-null  float64\n",
      " 30  fips                        96434 non-null   float64\n",
      "dtypes: float64(14), int64(2), object(15)\n",
      "memory usage: 29.3+ MB\n",
      "Rows: 123849\n",
      "Columns: 31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total Entry Level Swe Jobs</th>\n",
       "      <th>Years Covered</th>\n",
       "      <th>Latest Year</th>\n",
       "      <th>Average Salary</th>\n",
       "      <th>Median Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>772</td>\n",
       "      <td>2024</td>\n",
       "      <td>2024</td>\n",
       "      <td>112420.493089</td>\n",
       "      <td>105635.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Total Entry Level Swe Jobs  Years Covered  Latest Year  Average Salary  \\\n",
       "0                         772           2024         2024   112420.493089   \n",
       "\n",
       "   Median Salary  \n",
       "0      105635.25  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. load the dataset \n",
    "df = pd.read_csv(\"data/00-raw/postings.csv\")\n",
    "\n",
    "# 2. the dataset is already tidy (Rows = job postings, Columns = job attributes, No nested structures)\n",
    "df.info()\n",
    "\n",
    "# 3. demonstrate the size of the dataset\n",
    "print(\"Rows:\", df.shape[0])\n",
    "print(\"Columns:\", df.shape[1])\n",
    "\n",
    "# 4. find out how much data is missing, where its missing, and if its missing at random or seems to have any systematic relationships in its missingness\n",
    "missing = df.isnull().sum().sort_values(ascending=False)\n",
    "missing_percentage = (missing / len(df)) * 100\n",
    "\n",
    "missing_summary = pd.DataFrame({\n",
    "    \"Missing Count\": missing,\n",
    "    \"Missing %\": missing_percentage\n",
    "})\n",
    "\n",
    "missing_summary.head(15)\n",
    "\n",
    "# 5. find and flag any outliers or suspicious entries\n",
    "df['listed_time'] = pd.to_datetime(df['listed_time'], unit='ms', errors='coerce')\n",
    "df['year'] = df['listed_time'].dt.year\n",
    "\n",
    "df['year'].value_counts().sort_index()\n",
    "\n",
    "df[df['year'] < 2010]\n",
    "df[df['year'] > 2025]\n",
    "\n",
    "df['normalized_salary'].describe()\n",
    "\n",
    "\n",
    "# 6. clean the data or demonstrate that it was already clean.  You may choose how to deal with missingness (dropna of fillna... how='any' or 'all') and you should justify your choice in some way\n",
    "# Keep only valid years\n",
    "df = df[(df['year'] >= 2020) & (df['year'] <= 2025)]\n",
    "# Keep only US jobs\n",
    "df = df[df['currency'] == 'USD']\n",
    "# Keep only entry-level jobs\n",
    "df = df[df['formatted_experience_level'].str.contains(\"Entry\", case=False, na=False)]\n",
    "# Keep only software engineering titles\n",
    "df = df[df['title'].str.contains(\"Software|Engineer|Developer\", case=False, na=False)]\n",
    "\n",
    "\n",
    "# 7. You will load raw data from `data/00-raw/`, you will (optionally) write intermediate stages of your work to `data/01-interim` and you will write the final fully wrangled version of your data to `data/02-processed`\n",
    "df.to_csv(\"data/02-processed/postings_processed.csv\", index=False)\n",
    "\n",
    "df.head()\n",
    "# 8. summary statistics\n",
    "summary_stats = pd.DataFrame({\n",
    "    \"Total Entry Level Swe Jobs\": [len(df)],\n",
    "    \"Years Covered\": [df[\"year\"].min()],\n",
    "    \"Latest Year\": [df[\"year\"].max()],\n",
    "    \"Average Salary\": [df[\"normalized_salary\"].mean()],\n",
    "    \"Median Salary\": [df[\"normalized_salary\"].median()]\n",
    "})\n",
    "summary_stats\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ethics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instructions: Keep the contents of this cell. For each item on the checklist\n",
    "-  put an X there if you've considered the item\n",
    "-  IF THE ITEM IS RELEVANT place a short paragraph after the checklist item discussing the issue.\n",
    "  \n",
    "Items on this checklist are meant to provoke discussion among good-faith actors who take their ethical responsibilities seriously. Your teams will document these discussions and decisions for posterity using this section.  You don't have to solve these problems, you just have to acknowledge any potential harm no matter how unlikely.\n",
    "\n",
    "Here is a [list of real world examples](https://deon.drivendata.org/examples/) for each item in the checklist that can refer to.\n",
    "\n",
    "[![Deon badge](https://img.shields.io/badge/ethics%20checklist-deon-brightgreen.svg?style=popout-square)](http://deon.drivendata.org/)\n",
    "\n",
    "### A. Data Collection\n",
    " - [X] **A.1 Informed consent**: If there are human subjects, have they given informed consent, where subjects affirmatively opt-in and have a clear understanding of the data uses to which they consent?\n",
    "\n",
    "> Example of how to use the checkbox, and also of how you can put in a short paragraph that discusses the way this checklist item affects your project.  Remove this paragraph and the X in the checkbox before you fill this out for your project\n",
    "\n",
    " - [X] **A.2 Collection bias**: Have we considered sources of bias that could be introduced during data collection and survey design and taken steps to mitigate those?\n",
    "> We are concerned that the data itself may be biased or incomplete. Not all entry-level software engineering jobs are posted online, since some people get jobs through referrals, internal hiring, or campus recruiting that does not appear on job boards. Because of this, it is difficult to collect data that fully represents the true number of available jobs, which may affect the accuracy of the comparison.\n",
    "\n",
    " - [ ] **A.3 Limit PII exposure**: Have we considered ways to minimize exposure of personally identifiable information (PII) for example through anonymization or not collecting information that isn't relevant for analysis?\n",
    " - [ ] **A.4 Downstream bias mitigation**: Have we considered ways to enable testing downstream results for biased outcomes (e.g., collecting data on protected group status like race or gender)?\n",
    "\n",
    "### B. Data Storage\n",
    " - [X] **B.1 Data security**: Do we have a plan to protect and secure data (e.g., encryption at rest and in transit, access controls on internal users and third parties, access logs, and up-to-date software)?\n",
    "> The data used in this project is publicly available and does not include any private information. It is stored locally during the analysis, and basic care is taken to avoid accidentally changing or sharing the files. Because the data is low risk, no special security measures are needed.\n",
    "\n",
    " - [ ] **B.2 Right to be forgotten**: Do we have a mechanism through which an individual can request their personal information be removed?\n",
    " - [ ] **B.3 Data retention plan**: Is there a schedule or plan to delete the data after it is no longer needed?\n",
    "\n",
    "### C. Analysis\n",
    " - [X] **C.1 Missing perspectives**: Have we sought to address blindspots in the analysis through engagement with relevant stakeholders (e.g., checking assumptions and discussing implications with affected communities and subject matter experts)?\n",
    "> This project looks at overall numbers and trends, so it does not include personal experiences from recent graduates or employers. Because of this, the results may not fully explain why certain trends happen, and they should be understood as showing general patterns rather than individual experiences.\n",
    "\n",
    " - [X] **C.2 Dataset bias**: Have we examined the data for possible sources of bias and taken steps to mitigate or address these biases (e.g., stereotype perpetuation, confirmation bias, imbalanced classes, or omitted confounding variables)?\n",
    "> The datasets used in this analysis may introduce bias because they rely on broad categories and assumptions. Not all Computer Science graduates are looking for software engineering jobs, and not all jobs labeled as “entry-level” are actually accessible to new graduates. These mismatches can affect the comparison between supply and demand, so the results should be interpreted as approximate trends rather than exact measurements.\n",
    "\n",
    " - [X] **C.3 Honest representation**: Are our visualizations, summary statistics, and reports designed to honestly represent the underlying data?\n",
    "> The results are presented in a way that avoids oversimplifying or exaggerating the data. Instead of focusing on single-year changes, the analysis looks at overall trends across multiple years to reduce the impact of short-term fluctuations. All figures and summaries are explained in context so readers understand what the data shows and what it does not show.\n",
    "\n",
    " - [ ] **C.4 Privacy in analysis**: Have we ensured that data with PII are not used or displayed unless necessary for the analysis?\n",
    " - [X] **C.5 Auditability**: Is the process of generating the analysis well documented and reproducible if we discover issues in the future?\n",
    "> The steps of the analysis, including data sources and processing methods, are documented so that the work can be checked or repeated by others if needed, which also helps maintain transparency and accountability.\n",
    "\n",
    "\n",
    "### D. Modeling\n",
    " - [ ] **D.1 Proxy discrimination**: Have we ensured that the model does not rely on variables or proxies for variables that are unfairly discriminatory?\n",
    " - [ ] **D.2 Fairness across groups**: Have we tested model results for fairness with respect to different affected groups (e.g., tested for disparate error rates)?\n",
    " - [X] **D.3 Metric selection**: Have we considered the effects of optimizing for our defined metrics and considered additional metrics?\n",
    "> The metrics used, such as the number of graduates and the number of entry-level job postings, were chosen because they directly relate to the research question. However, these metrics do not capture job quality or underemployment, which is a limitation.\n",
    "\n",
    " - [ ] **D.4 Explainability**: Can we explain in understandable terms a decision the model made in cases where a justification is needed?\n",
    " - [X] **D.5 Communicate limitations**: Have we communicated the shortcomings, limitations, and biases of the model to relevant stakeholders in ways that can be generally understood?\n",
    "> The final results clearly explain the limitations of the analysis, including data gaps and simplifying assumptions, so readers understand what the results do and do not show.\n",
    "\n",
    "### E. Deployment\n",
    " - [X] **E.1 Monitoring and evaluation**: Do we have a clear plan to monitor the model and its impacts after it is deployed (e.g., performance monitoring, regular audit of sample predictions, human review of high-stakes decisions, reviewing downstream impacts of errors or low-confidence decisions, testing for concept drift)?\n",
    "> This analysis is based on data from a specific time period. If it were updated in the future, the data and methods would need to be checked again to make sure they still reflect current job market conditions.\n",
    "\n",
    " - [ ] **E.2 Redress**: Have we discussed with our organization a plan for response if users are harmed by the results (e.g., how does the data science team evaluate these cases and update analysis and models to prevent future harm)?\n",
    " - [ ] **E.3 Roll back**: Is there a way to turn off or roll back the model in production if necessary?\n",
    " - [X] **E.4 Unintended use**: Have we taken steps to identify and prevent unintended uses and abuse of the model and do we have a plan to monitor these once the model is deployed?\n",
    "> There is a possibility that the results could be misunderstood or used to discourage students from majoring in Computer Science. To reduce this risk, the analysis emphasizes that it describes overall trends in the job market and does not predict individual outcomes or career success.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* *We will communicate primarily through messages for quick updates and questions, and use Google Docs for longer-form work and progress tracking. We will meet once per week (virtually) and schedule additional meetings as needed*\n",
    "* *We agree to communicate in a blunt but polite manner. Team members should feel comfortable expressing disagreement or concerns respectfully and constructively*\n",
    "* *For major project decisions, we will aim for consensus. Otherwise, by majority vote. If in cases of time-sensitive decisions, the member will have to make a temporary decision at the moment and inform the group*\n",
    "* *Tasks will be divided based on individual choices as they want. We will track tasks and progress using a shared document, so responsibilities and deadlines are visible to everyone.*\n",
    "* *We will follow the agreed-upon project timeline and update it as needed. Team members are expected to complete assigned tasks by internal deadlines so the group can double-check.*\n",
    "* *If a team member is struggling to complete a task, they should notify the group as early as possible, so we can work together to redistribute work temporarily or provide support. If a member consistently misses deadlines without communication, the group will address the issue directly and follow course guidelines if needed.*\n",
    "* *All team members are expected to contribute equally in effort, communicate regularly, and respect each other’s time and commitments. We recognize that everyone has different strengths, schedules, and working styles, and we will support one another to ensure the project progresses smoothly.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/4  |  1 PM | Review COGS 108 project expectations; brainstorm project ideas related to Big Tech hiring  | Determine best form of communication; Discuss and decide on final research question; discuss hypothesis; begin background research | \n",
    "| 2/11  |  10 AM |  Do background research on CS graduate trends and Big Tech hiring patterns | Identify potential datasets (education + job postings) and ethics; draft project proposal | \n",
    "| 2/18  | 10 AM  | Edit, finalize, and submit proposal; Search for datasets  | Discuss Wrangling and possible analytical approaches; Assign group members to lead each specific part   |\n",
    "| 2/25  | 6 PM  | Import & Wrangle Data; EDA | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 3/4  | 12 PM  | Finalize wrangling/EDA; Begin Analysis | Discuss/edit Analysis; Complete project check-in |\n",
    "| 3/11  | 12 PM  | Complete analysis; Draft results/conclusion/discussion| Discuss/edit full project |\n",
    "| 3/18  | Before 11:59 PM  | double check assigned parts is completed and polished | Turn in Final Project & Group Project Surveys |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "16b860a9f5fc21240e9d88c0ee13691518c3ce67be252e54a03b9b5b11bd3c7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
